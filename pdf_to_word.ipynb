{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"\"\"It is unlikely that strategies learnt in this way will generalize to random perturbations;\n",
    "therefore the algorithm was only evaluated on the highest scoring single episode. In contrast, our\n",
    "algorithm is evaluated on \u000f-greedy control sequences, and must therefore generalize across a wide\n",
    "variety of possible situations. Nevertheless, we show that on all the games, except Space Invaders,\n",
    "not only our max evaluation results (row 8), but also our average results (row 4) achieve better\n",
    "performance.\n",
    "Finally, we show that our method achieves better performance than an expert human player on\n",
    "Breakout, Enduro and Pong and it achieves close to human performance on Beam Rider. The games\n",
    "Q*bert, Seaquest, Space Invaders, on which we are far from human performance, are more chal-\n",
    "lenging because they require the network to find a strategy that extends over long time scales.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is unlikely that strategies learnt in this way will generalize to random perturbations; therefore the algorithm was only evaluated on the highest scoring single episode. In contrast, our algorithm is evaluated on \u000f-greedy control sequences, and must therefore generalize across a wide variety of possible situations. Nevertheless, we show that on all the games, except Space Invaders, not only our max evaluation results (row 8), but also our average results (row 4) achieve better performance. Finally, we show that our method achieves better performance than an expert human player on Breakout, Enduro and Pong and it achieves close to human performance on Beam Rider. The gameQ*bert, Seaquest, Space Invaders, on which we are far from human performance, are more chal- lenging because they require the network to find a strategy that extends over long time scales. \n"
     ]
    }
   ],
   "source": [
    "s= s.replace('s\\n','')\n",
    "s=s.replace('\\n',' ')\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "08a8f7c9b23c59fcda95642d8863587fab6c74df0dd3b7c9d9c16ef8094e1120"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
